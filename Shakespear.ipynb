{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f241a40",
   "metadata": {},
   "source": [
    "Project Shakespear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b74bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5300e",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07f6d769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1115394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "print(\"Length of dataset:\", len(text))\n",
    "text[:500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fb65d",
   "metadata": {},
   "source": [
    " ## Tokenisation\n",
    "\n",
    "For this project, we wish to train a Language Model. Such model learns how to predict the next unit of text (word or character). Here, we choose a character-level model because the vocabulary is smaller. It is also simpler and learns raw linguistic structure. However, it is harder for the model to learn semantics.\n",
    "\n",
    "Before training the model, we need to convert the text into numerical tokens.\n",
    "Here, one token = one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba74758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieves all the set of characters in text, converts them into a list, and sorts the list.\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "#Creates two maps (to go from text to integers and vice versa).\n",
    "stoi = {ch: i for i, ch in enumerate(chars)} #string -> int\n",
    "itos = {i: ch for i, ch in enumerate(chars)} #int -> string\n",
    "\n",
    "# For encoding and decoding\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107834f",
   "metadata": {},
   "source": [
    "## Creating the batches\n",
    "\n",
    "We split the dataset in (random) batches.\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- block_size is the length of the \"context\" seen by the model. Each batch will contain only this number of characters. block_size should be chosen carefully as:\n",
    "    1. A too short context means loss of information.\n",
    "    2. A too long context makes the model heavy and slower.\n",
    "\n",
    "- batch_size is the number of sequences (samples) processed in parallel during training. As seen during the lessons, dividing the data into batches is necessary for different reasons, in particular  performance. Indeed, to evaluate a module on a sample, both the module's parameters and the sample must be copied into cache memory, which is fast but small. Memory transfers are slower than computation. Batch processing allows us to cut down to one copy of the parameters to the cache per batch. \n",
    "\n",
    "- Randomly choosing the sequences avoids that the model sees always the same sequences. It improves generalization, increases the diversity of the examples, avoids learning consecutive sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "516c3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "def get_batch(split):\n",
    "    #Split data into training set and testing/validation set\n",
    "    data_split = data[:int(0.9*len(data))] if split==\"train\" else data[int(0.9*len(data)):]\n",
    "    \n",
    "    #Randomly selects the initial indices for each batch.\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    # We store the inputs to the model\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    #Stores the input batches shifted by one index. This corresponds to the desired prediction/targets/labels (corresponds to next-character prediction learning).\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6444caf",
   "metadata": {},
   "source": [
    "# Implémentation du Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda094f4",
   "metadata": {},
   "source": [
    "### Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ebfe44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = torch.softmax(weights, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = weights @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c0a7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.proj(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4456423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a50c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b23dda",
   "metadata": {},
   "source": [
    "# Modèle complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d0b4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            logits.view(-1, vocab_size), \n",
    "            targets.view(-1)\n",
    "        )\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83482430",
   "metadata": {},
   "source": [
    "# Entraînement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94354617",
   "metadata": {},
   "source": [
    "Nous pouvons analyser l'évolution de la fonction loss par rapport à la quantité d'entraînement effectuée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b67fb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/6000 - loss: 4.3318\n",
      "Step 500/6000 - loss: 2.2966\n",
      "Step 1000/6000 - loss: 1.9679\n",
      "Step 1500/6000 - loss: 1.7645\n",
      "Step 2000/6000 - loss: 1.6024\n",
      "Step 2500/6000 - loss: 1.5574\n",
      "Step 3000/6000 - loss: 1.5038\n",
      "Step 3500/6000 - loss: 1.4704\n",
      "Step 4000/6000 - loss: 1.4153\n",
      "Step 4500/6000 - loss: 1.4005\n",
      "Step 5000/6000 - loss: 1.3860\n",
      "Step 5500/6000 - loss: 1.3247\n"
     ]
    }
   ],
   "source": [
    "model = TransformerLanguageModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "max_steps = 6000\n",
    "losses=[]\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    losses.append(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Step {step}/{max_steps} - loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c1f438",
   "metadata": {},
   "source": [
    "# Génération de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17c84d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So brave of you.\n",
      "\n",
      "First Servant:\n",
      "Well, so, is I know he, being of yours\n",
      "To this good more-dready given so obbedies;\n",
      "Greate our heaven, fair, take the fool?\n",
      "\n",
      "RIOLAND:\n",
      "Sir, dangerous; to your tongue\n",
      "To the mould entaintly of argement. Come,\n",
      "death, why, never they passantiates! We will bone life,\n",
      "Both the Duke of Hastings and first\n",
      "Entwitted me with when to palace's leave were.\n",
      "For, the etter advises, the more watch'd at marr'd.\n",
      "Who, most, to importune heart,' the looks of so,\n",
      "His no brot. But we'll or kevers, it she? he it least,\n",
      "do I'll have saved much do; yet far meth\n",
      "And discaled unreded with mistress works\n"
     ]
    }
   ],
   "source": [
    "def generate(model, start=\"O God, O God!\", max_new_tokens=300):\n",
    "    \"\"\"Inputs:\n",
    "    - model\n",
    "    - start\n",
    "    - max_new_tokens\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode(start), dtype=torch.long)[None, :]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(idx[:, -block_size:])\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        idx = torch.cat((idx, next_id), dim=1)\n",
    "\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "print(generate(model, start=\"So brave of you\", max_new_tokens=600))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912a5d3",
   "metadata": {},
   "source": [
    "### Training with n=8000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on 8000\n",
    "\n",
    "model = TransformerLanguageModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "max_steps = 8000\n",
    "losses=[]\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    losses.append(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Step {step}/{max_steps} - loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation of text\n",
    "\n",
    "def generate(model, start=\"O God, O God!\", max_new_tokens=300):\n",
    "    \"\"\"Inputs:\n",
    "    - model\n",
    "    - start\n",
    "    - max_new_tokens\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode(start), dtype=torch.long)[None, :]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(idx[:, -block_size:])\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        idx = torch.cat((idx, next_id), dim=1)\n",
    "\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "print(generate(model, start=\"So brave of you\", max_new_tokens=600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
