{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f241a40",
   "metadata": {},
   "source": [
    "Project Shakespear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae2bcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\melod\\OneDrive\\Bureau\\Université\\MASTER\\DEEPLEARNING\\MiniProject2\\Project2\\venv\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5300e",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f6d769",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "print(\"Length of dataset:\", len(text))\n",
    "text[:500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fb65d",
   "metadata": {},
   "source": [
    " ## Tokenisation\n",
    "\n",
    "For this project, we wish to train a Language Model. Such model learns how to predict the next unit of text (word or character). Here, we choose a character-level model because the vocabulary is smaller. It is also simpler and learns raw linguistic structure. However, it is harder for the model to learn semantics.\n",
    "\n",
    "Before training the model, we need to convert the text into numerical tokens.\n",
    "Here, one token = one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba74758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieves all the set of characters in text, converts them into a list, and sorts the list.\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "#Creates two maps (to go from text to integers and vice versa).\n",
    "stoi = {ch: i for i, ch in enumerate(chars)} #string -> int\n",
    "itos = {i: ch for i, ch in enumerate(chars)} #int -> string\n",
    "\n",
    "# For encoding and decoding\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107834f",
   "metadata": {},
   "source": [
    "## Creating the batches\n",
    "\n",
    "We split the dataset in (random) batches.\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- block_size is the length of the \"context\" seen by the model. Each batch will contain only this number of characters. block_size should be chosen carefully as:\n",
    "    1. A too short context means loss of information.\n",
    "    2. A too long context makes the model heavy and slower.\n",
    "\n",
    "- batch_size is the number of sequences (samples) processed in parallel during training. As seen during the lessons, dividing the data into batches is necessary for different reasons, in particular  performance. Indeed, to evaluate a module on a sample, both the module's parameters and the sample must be copied into cache memory, which is fast but small. Memory transfers are slower than computation. Batch processing allows us to cut down to one copy of the parameters to the cache per batch. \n",
    "\n",
    "- Randomly choosing the sequences avoids that the model sees always the same sequences. It improves generalization, increases the diversity of the examples, avoids learning consecutive sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516c3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "def get_batch(split):\n",
    "    #Split data into training set and testing/validation set\n",
    "    data_split = data[:int(0.9*len(data))] if split==\"train\" else data[int(0.9*len(data)):]\n",
    "    \n",
    "    #Randomly selects the initial indices for each batch.\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    # We store the inputs to the model\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    #Stores the input batches shifted by one index. This corresponds to the desired prediction/targets/labels (corresponds to next-character prediction learning).\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6444caf",
   "metadata": {},
   "source": [
    "# Implémentation du Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda094f4",
   "metadata": {},
   "source": [
    "### Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebfe44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = torch.softmax(weights, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = weights @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0a7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.proj(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4456423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a50c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b23dda",
   "metadata": {},
   "source": [
    "# Modèle complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0b4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            logits.view(-1, vocab_size), \n",
    "            targets.view(-1)\n",
    "        )\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83482430",
   "metadata": {},
   "source": [
    "# Entraînement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94354617",
   "metadata": {},
   "source": [
    "Nous pouvons analyser l'évolution de la fonction loss par rapport à la quantité d'entraînement effectuée.\n",
    "\n",
    "- optimizer Adaw - > Expliquer pourquoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b67fb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/6000 - loss: 4.3003\n",
      "Step 500/6000 - loss: 2.3235\n",
      "Step 1000/6000 - loss: 2.0010\n",
      "Step 1500/6000 - loss: 1.8090\n",
      "Step 2000/6000 - loss: 1.6581\n",
      "Step 2500/6000 - loss: 1.5803\n",
      "Step 3000/6000 - loss: 1.4747\n",
      "Step 3500/6000 - loss: 1.4740\n",
      "Step 4000/6000 - loss: 1.4226\n",
      "Step 4500/6000 - loss: 1.3389\n",
      "Step 5000/6000 - loss: 1.3633\n",
      "Step 5500/6000 - loss: 1.3283\n"
     ]
    }
   ],
   "source": [
    "model = TransformerLanguageModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "max_steps = 6000\n",
    "losses=[]\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    losses.append(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Step {step}/{max_steps} - loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c1f438",
   "metadata": {},
   "source": [
    "# Génération de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17c84d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So brave of your chides: I'll hear my goodness;\n",
      "But cure among time, like a weagh close, rafter\n",
      "From the next welcome, amonsel I had,\n",
      "For my slavin and your ignorance:\n",
      "Awhile, let's tendering injustance, term thou, as your faol,\n",
      "be to wine fair own accurte's business.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Make his comfort, be rest you woman,\n",
      "What some water thou legs ear of joints,\n",
      "Banishamable Edwart his head-feased under\n",
      "Blood having resprainy, knindest hath these Trouble grace,\n",
      "Ereck a frush of weak strengther's queence;\n",
      "And falow in my possess' well at leave me\n",
      "His mostingue.\n",
      "\n",
      "GREMIO:\n",
      "If with your joints and my back accus\n"
     ]
    }
   ],
   "source": [
    "def generate(model, start=\"O God, O God!\", max_new_tokens=300):\n",
    "    \"\"\"Inputs:\n",
    "    - model\n",
    "    - start\n",
    "    - max_new_tokens\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode(start), dtype=torch.long)[None, :]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(idx[:, -block_size:])\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        idx = torch.cat((idx, next_id), dim=1)\n",
    "\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "print(generate(model, start=\"So brave of you\", max_new_tokens=600))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912a5d3",
   "metadata": {},
   "source": [
    "### Training with n=8000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on 8000\n",
    "\n",
    "model = TransformerLanguageModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "max_steps = 8000\n",
    "losses=[]\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    losses.append(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Step {step}/{max_steps} - loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation of text\n",
    "\n",
    "def generate(model, start=\"O God, O God!\", max_new_tokens=300):\n",
    "    \"\"\"Inputs:\n",
    "    - model\n",
    "    - start\n",
    "    - max_new_tokens\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode(start), dtype=torch.long)[None, :]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(idx[:, -block_size:])\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        idx = torch.cat((idx, next_id), dim=1)\n",
    "\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "print(generate(model, start=\"So brave of you\", max_new_tokens=600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
